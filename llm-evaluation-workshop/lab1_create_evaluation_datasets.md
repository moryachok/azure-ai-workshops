# Create evaluation datasets

In this Lab you will learn how to prepare evaluation datasets.
These datasets are crucial for later steps where you are going to use  another Large Language Model (LLM) to evaluate quality of responses of other models - this process called **model-as-a-judge** and it allows you to dramatically scale your evaluation process and run it in a fully automated way. 

For the sake of this lab we created for you sample scenario. You are going to evaluate how your LLM models are summarizing call-center transcripts. This is a common use case for LLM to handle.

You can find sample call transcripts in the [transcripts](./transcripts/) folder.